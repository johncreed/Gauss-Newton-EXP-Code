\mychapter{Newton Method for Unconstrained Minimization }\label{sec:NewtonMin}
To derive the Newton method, we should re-write the function to have a vector of variables. To this end, the objective function in \eqref{eq:min_reMF} is changed to 
\begin{equation*}
f(\vectorize(U), \vectorize(V)),
\end{equation*}
where $\vectorize(\cdot)$ stacks all columns of a matrix to a vector.
For the standard Newton methods, at the $k$th iteration, we find a direction $\bs_k$ minimizing the following second-order approximation of the function value:
\begin{equation}
    \min_{\bs} \quad \frac{1}{2}\bs^T H_k \bs + \bg_k^T \bs
\label{eq:QP}
\end{equation}
where 
\begin{equation*}
H_k = \nabla^2 f(\vectorize(U_k), \vectorize(V_k))\text{ and }
\bg_k = \nabla f(\vectorize(U_k), \vectorize(V_k))
\end{equation*}
are the Hessian matrix and the gradient of $f(U_k, V_k)$, respectively.
If $H_k$ is positive definite, then \eqref{eq:QP} is equivalent to solving the following linear system:
\begin{equation}
    H_k\bs = -\bg_k
\label{eq:HLE}
\end{equation}

For non-convex objective functions, the Hessian matrix may not be positive definite. Then the solution of \eqref{eq:HLE} may not lead to a descent direction. In Section~\ref{sec:ANT} we will show that this issue occurs for our optimization problem \eqref{eq:reMF}. To obtain a descent direction, we can consider a positive-definite approximation of the Hessian matrix.

\par Now assume that $G_k$ is a positive definite approximation of the Hessian matrix. Instead of solving \eqref{eq:HLE}, we solve the following linear system to find a direction $\bs_k$.
\begin{equation}
	G_k\bs = -\bg_k
\label{eq:GLE}
\end{equation}

In practice, \eqref{eq:GLE} may not need to exactly solved. Therefore, truncated Newton methods have been introduced to approximately solve \eqref{eq:GLE}. Iterative methods such as conjugate gradient method are often used for approximately solving \eqref{eq:GLE}, so at each iteration an inner iterative process is involved. In particular, if CG is is used, at each inner iteration a matrix-vector product between $G_k$ and a vector must be conducted.

After a direction is found in our Newton method, we must decide the step size taken along that direction. A common setting is the backtracking line search. Namely, we find the largest step size $\theta\in\{1,\beta,\beta^2,\dots\}$ satisfying the following sufficient decrease condition.
Let 
\begin{equation}
\bs = \vectorize(S)
\label{eq:s}
\end{equation} 
and 
\begin{equation}
S = \begin{bmatrix} S_u & S_v \end{bmatrix}
\label{eq:susv}.
\end{equation}  
\begin{equation}
    f(U+\theta S_{u},V+\theta S_{v}) - f(U,V) \le \theta\nu {\bg}^T\vectorize(S),
    \label{eq:LineSearchRule}
\end{equation}
where $\nu\in(0,1)$ and $\beta\in(0,1)$ are pre-specified constants.