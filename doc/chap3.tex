\mychapter{Newton Methods for Factorization Machines}\label{sec:NewtonFM}
To compute the gradient and Hessian of \eqref{eq:reMF}, we begin with the following properties of  the Kronecker product among matrices $A$, $B$ and $X$:
\begin{equation}
(B^T \otimes A)\vectorize(X) = \vectorize(AXB)
\label{eq:kronecker_vec}
\end{equation}
\begin{equation}
(A \otimes B)^T = A^T \otimes B^T
\label{eq:kronecker_T}.
\end{equation}
Moreover, the predicted value can be represented as
\begin{align}
(U \bw_i)^T (V \bh_i) &= (\bh_i^T \otimes (U \bw_i)^T)\vectorize(V)\label{eq:predict_UwVh_T} \\
&= (\bh_i \otimes (U \bw_i))^T\vectorize(V)
\label{eq:predict_UwVh},
\end{align}
where \eqref{eq:predict_UwVh_T} and \eqref{eq:predict_UwVh} are from \eqref{eq:kronecker_vec} and \eqref{eq:kronecker_T}, respectively.
The predicted value also can be represented as
\begin{align}
(U \bw_i)^T (V \bh_i) &= (V \bh_i)^T (U \bw_i) \nonumber \\
&= (\bw_i^T \otimes (V \bh_i)^T)\vectorize(U)\nonumber \\
&= (\bw_i \otimes (V \bh_i))^T \vectorize(U)
\label{eq:predict_VhUw}.
\end{align}
Then, we define 
\begin{equation}
\bp_i = U\bw_i ,
\bq_i = V\bh_i ,
%z_i = \bp_i^T\bq_i ,
\tilde{y}_i = \bp_i^T\bq_i,
b_i = \tilde{y}_i-y_i,
\label{eq:pqyb}
\end{equation}
the loss function 
\begin{equation*}
\xi_i = b_i^2 = \Bigl(\bp_i^T\bq_i-y_i\Bigr)^2, 
\end{equation*}
\begin{align*}
Q=[\bq_1,\dots,\bq_l] \in \bbr^{d\times l},
P=[\bp_1,\dots,\bp_l] \in \bbr^{d\times l},
\end{align*}
\begin{equation}
W=[\bw_1,\dots,\bw_l]^T \in \bbr^{l\times M},
H=[\bh_1,\dots,\bh_l]^T \in \bbr^{l\times N}.
\label{eq:WandH}
\end{equation}
\mysection{Gradient Calculation}
The gradient of \eqref{eq:reMF} is
\begin{align}
\nabla {f}(\vectorize(U, V)) = \lambda \vectorize(U,V) + \frac{1}{2}\sum_{i=1}^{l}\frac{\partial \xi_i}{\partial \tilde{y}_i}\begin{bmatrix} \frac{\partial \tilde{y}_i}{\partial \vectorize(U)} \\ \frac{\partial \tilde{y}_i}{\partial \vectorize(V)} \end{bmatrix}
\label{eq:gradUV}.
\end{align}
From \eqref{eq:predict_VhUw}, \eqref{eq:predict_UwVh} and \eqref{eq:pqyb},we define the following Jocobian of $\tilde{y}_i$ as
\begin {align}
%\bj_i = (\bw_i\otimes \bq_i, \bh_i\otimes \bp_i)
\bj_i &= \begin{bmatrix} \frac{\partial \tilde{y}_i}{\partial \vectorize(U)} \\ \frac{\partial \tilde{y}_i}{\partial \vectorize(V)} \end{bmatrix}\nonumber \\
&= \begin{bmatrix} \frac{\partial (\bw_i \otimes (V \bh_i))^T\vectorize(U)}{\partial \vectorize(U)} \\ \frac{\partial (\bh_i \otimes (U \bw_i))^T\vectorize(V)}{\partial \vectorize(V)} \end{bmatrix}\nonumber \\
&= \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix}
%&= \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix}_{(Md+Nd)\times 1}
\label{eq:Jacob}.
\end{align}
From \eqref{eq:gradUV} and \eqref{eq:Jacob}, the gradient is
\begin {align}
\nabla {f}(\vectorize(U, V)) 
&= \lambda \vectorize(U,V) + \sum_{i=1}^l b_i \bj_i\nonumber\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \sum_{i=1}^l b_i (\bw_i\otimes\bq_i) \\ \sum_{i=1}^l b_i (\bh_i\otimes\bp_i)\end{bmatrix}\label{eq:GradU_K1}\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \sum_{i=1}^l \vectorize\left(\bq_i b_i \bw_i^T \right) \\ \sum_{i=1}^l \vectorize\left(\bp_i b_i \bh_i^T \right)\end{bmatrix}\label{eq:GradU_K2}\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \vectorize\left( \sum_{i=1}^l \bq_i b_i \bw_i^T \right) \\ \vectorize\left( \sum_{i=1}^l \bp_i b_i \bh_i^T \right)\end{bmatrix}\nonumber\\
&= \lambda \vectorize(U,V) + \begin{bmatrix} \vectorize \Bigl ( Q \bigl(\diag(\bsym{b}) W \bigr)  \Bigr) \\ \vectorize \Bigl ( P \bigl(\diag(\bsym{b}) H \bigr)  \Bigr)\end{bmatrix}
\label{eq:Grad},
\end{align}
where $\diag(\bsym{b})$ is a diagonal matrix in which the $i$th diagonal element is $b_i$, and for the second term in \eqref{eq:GradU_K2} we apply \eqref{eq:kronecker_vec}. 
Usually $W$ and $H$ is highly sparse matrices. The cost of gradient calculation is
\begin{equation*}
\bbO{d\times (\text{nnz}(W)+\text{nnz}(H))},
\end{equation*}
where $\text{nnz}(\cdot)$ is the number of non-zero elements in a sparse matrix.
\par
\mysection{Hessian and Gauss-Newton Matrices}
The Hessian matrix of \eqref{eq:reMF} is 
\begin{align}
\begin{aligned}
&\frac{\partial }{\partial \vectorize(U,V)^T} \frac{\partial f}{\partial \vectorize(U,V)}\\
=&\lambda I + \sum_{i=1}^{l}\frac{\partial }{\partial \vectorize(U,V)^T} (b_i \bj_i)\\
=&\lambda I + \sum_{i=1}^{l} \Bigl( \frac{\partial }{\partial b_i^T} (b_i \bj_i) \frac{\partial b_i}{\partial \vectorize(U,V)^T} + \frac{\partial }{\partial \bj_i^T} (b_i \bj_i) \frac{\partial \bj_i}{\partial \vectorize(U,V)^T} \Bigr)\\
=&\lambda I + \sum_{i=1}^{l} \Bigl( \bj_i \frac{\partial b_i}{\partial \tilde{y}_i} \frac{\partial \tilde{y}_i}{\partial \vectorize(U,V)^T} + b_i I \frac{\partial \bj_i}{\partial \vectorize(U,V)^T} \Bigr)\\
=&\lambda I + \sum_{i=1}^{l} \bj_i \bj_i^T + \sum_{i=1}^{l} b_i \frac{\partial \bj_i}{\partial \vectorize(U,V)^T}, 
	\label{eq:H}
\end{aligned}
\end{align}
where $I$ is the identity matrix. This matrix may not be positive definite.
We remove the last term in \eqref{eq:H} and obtain the following Gauss-Newton matrix, which is positive definite.
\begin{equation}
    \begin{aligned}
    G &= \lambda I + \sum_{i=1}^{l}\bj_i\bj_i^T
    \label{eq:G}
    \end{aligned}
\end{equation}

To get the product between the Gauss-Newton matrix and a vector 
\begin{equation*}
\bs = \vectorize(S) = \begin{bmatrix}\vectorize(S_u)\\ \vectorize(S_v)\end{bmatrix}, 
\end{equation*}
from (\ref{eq:G}), (\ref{eq:Jacob}) and (\ref{eq:kronecker_vec}), we first calculate
\begin{equation}
\begin{aligned}
    z_i&= \bj_i^T \vectorize(S)\\
    &=\begin{bmatrix} \bw_i^T\otimes\bq_i^T & \bh_i^T\otimes\bp_i^T \end{bmatrix}\begin{bmatrix}\vectorize(S_u)\\ \vectorize(S_v)\end{bmatrix}\\
    &=\bq_i^T S_u \bw_i + \bp_i^T S_v \bh_i, \text{ }i=1,\dots,l
    \label{eq:DefZi},
\end{aligned}
\end{equation}
or equivalently
\begin{equation}
    \textbf{\textit{z}} = \biggl({Q^T}\odot{(WS_u^T)}+{P^T}\odot{(HS_v^T)}\biggr) \bsym{1}_{d\times 1}
    \label{eq:CalcZ},
\end{equation}
where $\odot$ is the Hadamard product (i.e., element-wise product) of two matrices.  Next, from (\ref{eq:DefZi}), (\ref{eq:G}), (\ref{eq:Jacob}), (\ref{eq:s}) and  (\ref{eq:kronecker_vec}),
\begin{align}
    G \bs         &= \lambda \vectorize(S) + \sum_{i=1}^l z_i \bj_i \nonumber\\
                  &= \lambda \vectorize(S) + \sum_{i=1}^l z_i \begin{bmatrix} \bw_i\otimes \bq_i \\ \bh_i\otimes \bp_i \end{bmatrix} \nonumber \\
                  &= \lambda \vectorize(S) +  \sum_{i=1}^l \begin{bmatrix}\vectorize\left(z_i \bq_i \bw_i^T\right) \\ \vectorize\left(z_i \bp_i \bh_i^T\right)\end{bmatrix} \nonumber  \\
                  &= \lambda \vectorize(S) +  \begin{bmatrix}\vectorize \Bigl ( Q \bigl(\diag(\textbf{\textit{z}}) W \bigr) \Bigr)\\\vectorize \Bigl ( P \bigl ( \diag(\textbf{\textit{z}}) H \bigr) \Bigr)\end{bmatrix} \label{eq:Hv}.
\end{align}

We investigate the complexity of the Hessian-vector product. The first term of 
\eqref{eq:Hv} is a standard vector scaling that costs \bbO{(M+N) \times d}.
For the second term in (\ref{eq:Hv}), we separately consider two major steps:
\begin{enumerate}
    \item The computation of $WS_u^T$ and $HS_v^T$: $\bbO{d \times (\text{nnz}(W)+\text{nnz}(H))}$
    \item The element-wise product between $P^T$ and $HS_v^T$:  $\bbO{ d \times l}$
    \item The product between dense vector $Q$ and $\diag{(\bsym{z})}W$: $\bbO{d \times l}$
\end{enumerate}
Therefore, the cost of one Hessian-vector  is $\bbO{d \times (\text{nnz}(W)+\text{nnz}(H)})$. 
\mysection{Implementation of Line Search}
\label{sec:implementLS}
Recalculating the function value at each $U+\theta S_{u}$ and $V+\theta S_{v}$ is expensive, where the main cost is on calculating $((U+\theta S_{u})\bw_i)^T((V+\theta S_{v})\bh_i)$, $\forall i$.
However, the following trick in \citet{GXY09a} can be employed.
Assume that
\begin{equation}
    \tilde{y}_i = (U\bw_i)^T(V\bh_i) 
    \label{eq:predicted_y},
\end{equation}
\begin{equation}
    {\Delta}_i=(S_u\bw_i)^T(V\bh_i)+(U\bw_i)^T(S_v\bh_i)   
    \label{eq:LsRequire_1}
\end{equation}
and
\begin{equation}
    {\Delta}_i^\prime = (S_u\bw_i)^T(S_v\bh_i),\; i=1,\dots,l    
    \label{eq:LsRequire_2}
\end{equation}
are available.
At an arbitrary $\theta$, we can calculate
\begin{equation}
    \left((U+\theta S_u)\bw_i\right)^T\left((V+\theta S_v)\bh_i\right) = \tilde{y}_i + \theta{\Delta}_i + {\theta}^2{\Delta}_i^\prime    
    \label{eq:ExpTheta}
\end{equation}
to get the new output value.
Now we have 
\begin{align}
        f(U+\theta S_{u},V+\theta S_{v}) - f(U,V) 
        &= \frac{\lambda}{2}\left(\|U+S_u\|_F^2+\|V+S_v\|_F^2-\|U\|_F^2-\|V\|_F^2\right)\nonumber\\
        &+ \frac{1}{2} \sum_{i=1}^l (y_i-\tilde{y}_i-\theta{\Delta}_i-{\theta}^2{\Delta}_i^\prime)^2 - \frac{1}{2} \sum_{i=1}^l (y_i-\tilde{y}_i)^2\nonumber\\
        &= \frac{\lambda}{2}\left( 2\theta \langle{U}{,}{S_u}\rangle + 2\theta \langle{V}{,}{S_v}\rangle + \theta^2\|S\|_F^2 \right)\nonumber\\ 
        &+ \frac{1}{2} \sum_{i=1}^l (y_i-\tilde{y}_i-\theta{\Delta}_i-{\theta}^2{\Delta}_i^\prime)^2 - \frac{1}{2} \sum_{i=1}^l (y_i-\tilde{y}_i)^2 \nonumber       
,
\end{align}
where $\langle\cdot{,}\cdot\rangle$ is the inner product of two matrices.
If we further maintain
\begin{equation}    
    \langle{U}{,}{S_u}\rangle,\langle{V}{,}{S_v}\rangle,\|S\|_F^2,\text{ and } {\bg}^T{\bs},
    \label{eq:LsRequire1}
\end{equation}
then the total cost of checking the condition in \eqref{eq:LineSearchRule} is merely $\bbO{l}$.
\mysection{Overall Procedure}
%\clearpage\newpage
%\section{Algorithm}
\begin{algorithm}[t]
    \caption{Newton method to solve \eqref{eq:reMF} by matrix-based operations.}
    \label{alg:LrFramework}
    \begin{algorithmic}[1]
        \State Given $0< \epsilon < 1$ and the initial $(U,V)$.        
        \State Compute and cache $\tilde{\by} = ((VH^T) \odot (UW^T))^T\bsym{1}_{d\times 1}$.
        \State $\bb \leftarrow \tilde{\by} - \by $
        %\State $f \gets \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2) + \frac{1}{2}\sum_{i=1}^l {y_i - \tilde{y}_i}$.
        \State $f \gets \frac{\lambda}{2}(\|U\|_F^2 + \|V\|_F^2) + \frac{1}{2}\bsym{b}^T \bsym{b}$.
        \For {$k \gets \{0,1,\dots\}$}
%            \State Calculate and store $\tilde{y}_i$ and then obtain $b_i$, $\forall i$.
            %\State Calculate and store $\tilde{y}_i$ and then obtain $b_i$ by \eqref{eq:LossD}, $\forall i$.
            \State $Q \gets VH^T$, $P \gets UW^T$
            \State Compute $G \gets \lambda \begin{bmatrix} U \, V \end{bmatrix} + \begin{bmatrix} Q \bigl(\diag(\bsym{b}) W \bigr) \, P \bigl ( \diag(\bsym{b}) H \bigr) \end{bmatrix}$.\label{alg:LrFramework:compG}
            \If {$k=0$}
                \State $\|G^0\|_F \gets \|G\|_F$
            \EndIf
            \If {$\|G\|_F \le \epsilon\|G^0\|_F$}
                \State Output $\begin{bmatrix} U \, V \end{bmatrix}$ as the solution of \eqref{eq:reMF}.
                \State {\bf break}
            \EndIf
            %\State Compute $D_{ii}$ via \eqref{eq:LossDD}, $\forall i$.
            \State Run CG in Algorithm \ref{alg:PCG} to get an update direction $S =  \begin{bmatrix} S_u \, S_v \end{bmatrix}$.
            \State Calculate\par 
            \begin{align}
            	\begin{split}
            	\bsym{\Delta} &= \left( Q^T \odot (WS_u^T)+ P^T\odot (HS_v^T) \right)\bsym{1}_{d\times 1},\nonumber\\
            	\bsym{\Delta}^\prime &= \left( (WS_u^T) \odot (HS_v^T) \right)\bsym{1}_{d\times 1}.
            	\end{split}
            \end{align}\label{alg:LrFramework:compD}
            \State Parpare the following values\par
            \begin{equation*}
            \langle{U}{,}{S_u}\rangle,\langle{V}{,}{S_v}\rangle,\|S\|_F^2,\text{ and } \langle{G}{,}{S}\rangle.
            \end{equation*}\label{alg:LrFramework:compGS}
            \For { $\theta\gets\{1,\beta,\beta^2,\dots\}$ }
                \State $\delta \gets \frac{\lambda}{2}\left( 2\theta \langle U, S_u \rangle + 2\theta \langle V, S_v \rangle + \theta^2\|S\|_F^2 \right) 
                + \frac{1}{2} \|\bsym{b}-\theta \bsym{\Delta}-{\theta}^2 \bsym{\Delta}^\prime \|^2
                - \frac{1}{2}\bsym{b}^T \bsym{b}$\label{alg:LrFramework:compd}
                \If { $ \delta \le \theta \nu \langle G,S \rangle $}
                    \State $U \gets U +\theta S_u$, $V \gets V +\theta S_v$
                    \State $f \gets f+ \delta$
                    \State $\tilde{\by} \gets \tilde{\by}+\theta\bsym{\Delta} +{\theta}^2 \bsym{\Delta}^\prime$
                    \State $\bb \leftarrow \tilde{\by} - \by $
                    \State {\bf break}
                \EndIf
            \EndFor
        \EndFor
    \end{algorithmic}
\end{algorithm}
The overall procedure is in Algorithm \ref{alg:LrFramework}. For the sack of efficiency, if possible, for each operation, we use a matrix-based form rather than a vector-based representation. We discuss some important ones. 

For the gradient, from the vector form in \eqref{eq:Grad}, we rewrite it as a matrix in Line \ref{alg:LrFramework:compG}. 
For the stopping condition, we follow past works on Newton methods such as \citet{CJL07b} to use
\begin{equation}
    \|\nabla f(\vectorize(U), \vectorize(V))\| \le \epsilon \|\nabla f(\vectorize(U_{\text{init}}),\vectorize(V_{\text{init}})\|,
    \label{eq:AntStop}
\end{equation}
where $0<\epsilon<1$ is a small stopping tolerance and $(U_{\text{init}},V_{\text{init}})$ indicates the initial point of the model. By the matrix form in Line \ref{alg:LrFramework:compG}, this stopping condition is essentially 
\begin{equation}
    \|\nabla f(U,V)\|_F \le \epsilon \|\nabla f(U_{\text{init}},V_{\text{init}})\|_F,
    \label{eq:AntStopM}
\end{equation}
where $\|\cdot\|_F$ is Frobenius norm of a matrix.
For ${\Delta}_i$, ${\Delta}_i^\prime$ in \eqref{eq:ExpTheta}, for line search, we aggregate all values together to have the form in Line \ref{alg:LrFramework:compD}. Besides, other values in \eqref{eq:LsRequire1} to be maintained are in Line \ref{alg:LrFramework:compGS}. The check of the sufficient decrease condition \eqref{eq:LineSearchRule} is by the calculation in Line \ref{alg:LrFramework:compd}.

For details of the CG procedure to approximately solve the linear system \eqref{eq:GLE}, we provide Algorithm \ref{alg:CG}. The main cost in each iteration of CG is the product between the Gauss-Newton matrix and a vector in Line \ref{alg:CG:compz} and \ref{alg:CG:compDh}, where we follow details in \eqref{eq:CalcZ} and \eqref{eq:Hv}. Its computational complexity is $\bbO{d \times (\text{nnz}(W)+\text{nnz}(H)}$ from what we discussed in \eqref{eq:Hv}.

After $\bs_k$ is obtained from the conjugate gradient method, a backtracking line search is applied to adjust the step size $\alpha$. From the discussion in Section~\ref{sec:implementLS}, the complexity of the line search is
\begin{equation*}
\bbO{(\text{\# of line search steps})\times  l}.
\end{equation*}
The overall complexity per Newton iteration is
\begin{equation}
(\text{\# of CG iterations}+2)\times \bbO{d \times (\text{nnz}(W)+\text{nnz}(H))}+(\text{\# of line search steps})\times \bbO{l},
\label{eq:overall_complexity}
\end{equation}
where the term $2\times \bbO{d \times (\text{nnz}(W)+\text{nnz}(H))}$ is from the cost in Line \ref{alg:LrFramework:compG} and Line \ref{alg:LrFramework:compD} in Algorithm \ref{alg:LrFramework} to compute \eqref{eq:Grad}, \eqref{eq:LsRequire_1} and \eqref{eq:LsRequire_2}.
If matrix factorization is considered, \eqref{eq:wi} and \eqref{eq:hi} imply that 
\begin{equation}
\text{nnz}(W)=\text{nnz}(H)= l.
\label{eq:nnzWnnzHl}
\end{equation}
Thus \eqref{eq:overall_complexity} can be written as
\begin{equation*}
(\text{\# of CG iterations}+2)\times \bbO{ld}+(\text{\# of line search steps})\times \bbO{l}.
\end{equation*}

\begin{algorithm}[t]
    \caption{A conjugate gradient method for solving \eqref{eq:GLE} by matrix-based operations.}
    \label{alg:CG}
    \begin{algorithmic}[1]
        \State Given $0<\eta<1$ and $G$, the gradient matrix form of \eqref{eq:Grad}. Let $S=\bsym{0}_{d\times (M+N)}$.
        \State Calculate $R = -G$, $D=R$, and $\gamma^0=\gamma=\|R\|_F^2$.
        \While {$\sqrt{\gamma} > \eta \sqrt{\gamma^0}$}
            \State $\bz \gets \left( {Q^T}\odot{(WD_{u}^T)}+{P^T}\odot{(HD_{v}^T)} \right) \bsym{1}_{d\times 1}$\label{alg:CG:compz}
            \State $D_h \gets \left(\lambda D + \begin{bmatrix} Q \bigl(\diag(\bz) W \bigr) \, P \bigl ( \diag(\bz) H \bigr) \end{bmatrix}\right) $\label{alg:CG:compDh}
            \State $\alpha \gets \gamma / \langle D,D_h \rangle$
            \State $S \gets S+\alpha D$
            \State $R \gets R-\alpha D_h$
            \State $\gamma^{\text{new}} \gets \|R\|_F^2$
            \State $\beta \gets \gamma^{\text{new}}/\gamma$
            \State $D \gets R+\beta D$
            \State $\gamma \gets \gamma^{\text{new}}$
        \EndWhile
        \State Output $S$ as the solution.
    \end{algorithmic}
\end{algorithm}

Next we discuss the memory usage. The main bottleneck occurs when conducting Line \ref{alg:CG:compz} of Algorithm \ref{alg:CG} or Line \ref{alg:LrFramework:compD} of Algorithm \ref{alg:LrFramework}. Take Line \ref{alg:CG:compz} of Algorithm \ref{alg:CG} as an example. We must store the following matrix 
\begin{equation}
P \in \bbr^{d \times l}, Q \in \bbr^{d \times l}, WD_{u}^T \in \bbr^{l \times d}, HD_{v}^T \in \bbr^{l \times d}, W \in \bbr^{l \times M},\text{ and }H \in \bbr^{l \times N}. 
\label{eq:severalmatrices}
\end{equation}
Thus the total memory consumption is
\begin{equation*}
4 \times \bbO{ld}+ \text{nnz}(W) + \text{nnz}(H).
\end{equation*}
For the case of matrix factorization, from \eqref{eq:nnzWnnzHl}, $\text{nnz}(W)$ and $\text{nnz}(H)$ are smaller, so the four $l \times d$ matrices are the bottleneck.
%Note that the step in Line \ref{alg:CG:compz} in Algorithm \ref{alg:CG} at least generates four $l \times d$ matrices. Because it may be out of memory, we can not use larger datasets.
\mysection{Implementation for Matrix Factorization}
From \eqref{eq:wi}, \eqref{eq:hi} and \eqref{eq:WandH}, for matrix factorization $W$ and $H$ are special matrices when each row contains exactly one non-zero entry. We explore how to take such properties to reduce the memory consumption.
\par Consider the two main operations \eqref{eq:CalcZ} and \eqref{eq:Hv} for the Gauss-Newton matrix-vector product. From
\begin{align*}
P = UW^T \text{ and } Q = VH^T,
\end{align*}
\eqref{eq:CalcZ} is the same as 
\begin{align*}
\textbf{\textit{z}} = \biggl({(HV^T)}\odot{(WS_u^T)}+{(WU^T)}\odot{(HS_v^T)}\biggr) \bsym{1}_{d\times 1}.
\end{align*}
Let $\textbf{\textit{z}}$ be indexed by $(m, n)$ in $R$ with $R_{m,n} \neq 0$, and
\begin{align*}
S_u=[\ensuremath{\bsym{S_u}}^1,\dots,\ensuremath{\bsym{S_u}}^M] \in \bbr^{d\times M},\\
S_v=[\ensuremath{\bsym{S_v}}^1,\dots,\ensuremath{\bsym{S_v}}^N] \in \bbr^{d\times N}.
\end{align*}
Then each component in \eqref{eq:CalcZ} is
\begin{equation*}
\textbf{\textit{z}}_{(m,n)} = \bv_n^T \ensuremath{\bsym{S_u}}^m + \bu_m^T \ensuremath{\bsym{S_v}}^n.
\end{equation*}
Let $Z \in \bbr^{M \times N}$ be a sparse matrix to include these $\textbf{\textit{z}}_{(m,n)}$ elements.
\par We note that 
\begin{equation*}
H^TW = I\left[R^T\right],
\end{equation*}
where $I\left[ \cdot \right]$ is an indicator function with values $1$ or $0$ to reflect non-zero position of the input matrix. Thus in the calculation of \eqref{eq:Hv} we have 
\begin{equation*}
\begin{aligned}
&Q \diag(\textbf{\textit{z}}) W \\
=& VH^T \diag(\textbf{\textit{z}}) W\\
=& V \bigl( Z^T \odot I\left[R^T\right] \bigr).
\end{aligned}
\end{equation*}
Further
\begin{equation*}
\begin{aligned}
&P \diag(\textbf{\textit{z}}) H \\
=& UW^T \diag(\textbf{\textit{z}}) H\\
=& U \bigl( Z \odot I\left[R\right] \bigr).
\end{aligned}
\end{equation*}
By the above settings, we never need to store any matrix in \eqref{eq:severalmatrices}. Therefore the memory consumption is significantly reduced to 
\begin{equation*}
 \bbO{\text{nnz}(R)+(M+N)d}.
\end{equation*}
For the computational complexity, it remains the same as the one shown in \eqref{eq:overall_complexity}. The reason is that 
\begin{equation*}
\text{nnz}(W)=\text{nnz}(H)= \text{nnz}(R).
\label{eq:nnzWnnzHnnzR}
\end{equation*}
\par We give details of other places in Algorithm \ref{alg:CG} that also need to be changed.
Let $B \in \bbr^{M \times N}$ contains elements $B_{m,n} = \bu_m^T \bv_n - r_{m,n}$.
At Line \ref{alg:LrFramework:compG} in Algorithm \ref{alg:LrFramework},we have
\begin{equation*}
G \gets \lambda \begin{bmatrix} U \, V \end{bmatrix} + \begin{bmatrix} V \bigl( B^T \odot I\left[R^T\right] \bigr) \, U \bigl( B \odot I\left[R\right] \bigr) \end{bmatrix}.
\end{equation*}
Then let $Z^\prime \in \bbr^{M \times N}$ contains elements $Z^\prime_{m,n} = (\ensuremath{\bsym{S_u}}^m)^T \ensuremath{\bsym{S_v}}^n$.
At Line \ref{alg:LrFramework:compD} in Algorithm \ref{alg:LrFramework}, we have
\begin{align*}
\bsym{\Delta} = Z,\\
\bsym{\Delta}^\prime = Z^\prime.
\end{align*}
